---
output: pdf_document
---

```{r global_options, include=FALSE}
library(knitr)
wd <- normalizePath("../../")
wd
opts_knit$set(fig.width=5, fig.height=5, fig.path='fig/',
               echo=FALSE, warning=FALSE, message=FALSE,
               root.dir=wd)
```

Binary Outcomes
===============

Binary outcomes are categorical outcome variables with exactly two categories, such as whether or not someone voted, whether two countries are at war, and so on. These variables are usually coded as $y_i \in \{0, 1\}$, with one representing "an event" and zero representing "a non-event." In generic language, we'll say that $y_i = 1$ means that "an event has occurred" and $y_i = 0$ means that "an event has not occurred." This allows us to talk about the "probability of an event."

Linear Probability Model
------------------------
We can use the normal-linear model (i.e., OLS) in this situation. If we do, we'll refer to it as the linear probability model (LPM).

Recall that we the linear model is represented by the equation $E(y_i) = X_i\beta$. It is important to note that a probability is just a particular kind of expected value--a probability is an expected value of a binary variable. Since $y_i$ is binary, the $E(y_i) = Pr(y_i)$, giving us $Pr(y_i) = X_i\beta$.

1. It is *very* easy to estimate (i.e., OLS; $\hat{\beta} = (X'X)^{-1}X'y$).
2. It is easy to interpret (i.e., a one unit change in $x_j$ leads to a $\hat{\beta_j}$ unit increase in $Pr(y)$).

However, the LPM has four weaknesses, that (usually) outweigh the strengths.

1. *Unbounded predictions.* Because the potential values for the expanatory variables are unbounded, you can obtain predicted probabilities above one and below zero. Of course, these predictions make no sense.
2. *Conditional heteroskedasticity.* The normal-linear model assumes a constant variance. However, it is impossible to have homoskedastic residuals of a binary outcome if the probability of an event varies. Specifically, if $y_i$ is binary, then $V(y_i) = Pr(y_i)[1 - Pr(y_i)]$, which, for the LPM, equals $X_i\beta(1 - X_i\beta)$. You'll notice that non-zero coefficients imply heteroskedasticity.
3. *Non-normal errors.* Normal errors implies that the residuals can take on any value along the real line, with values closer to zero being more likely and errors outside three standard deviations being quite unlikely. However, if $y_i$ is binary, then the residual can take on only two values: $-Pr(y_i)$ or $1 - Pr(y_i)$.
4. *Functional form.* Theoretically, you'd probably expect explanatory variables to have smaller effects as $Pr(y_i)$ approaches zero or one. The LPM assumes that the effects are constant.

Given these problems we'll want an approach to modeling binary outcomes that maps onto our knowledge about the process.

Logit and Probit
----------------

We'll use two models known as logit (also known as logistic regression) and probit to deal with binary outcomes. There are lots of variants of these two, but logit and probit are basically indistinguishable. Logit comes out of statistics and probit comes from econometrics. You'll see why economists and statisticians use different models in the derivations below.

### Probability Modeling Approach

In PSC 531, we relied almost exclusively on the model $y_i \sim N(\mu_i, \sigma^2)$, where $\mu_i = X_i\beta$. Toward the end of class, we mentioned that we can obtain a much broader class of models by replacing the normal distribution $N$ with a generic distribution $f$ and applying a function $g$ to $mu_i$. This gives us a generic model $y_i \sim f(\theta_i)$, where $\theta_i = g^{-1}(X_i\beta)$. (Notice that I'm supressiong the $\sigma^2$--sometimes we'll need it and sometimes we won't.)

To model a binary outcome, we'll need to choose something besides a normal distribution for our $f$. This is an easy choice, because there is exactly one possible distribution for a binary variable--Bernoulli. 

Recall that the pmf for the Bernoulli distribution is given by $Bernoulli(y_i | \pi_i) = \pi_i^{y_i}(1 - \pi_i)^{1 - y_i}$. 

>#### An Aside on Notation
>Notice that from the Normal to the generic to the Bernoulli, I've gone from $\mu_i$ to $\theta_i$ to $\pi_i$ to basically represent the same idea. It is fairly convential to represent the parameter of the generic model with $\theta$, and the specific distributions have their conventions as well. For example it is common to use $\mu$ to represent the mean paramater of the normal distribution and $\pi$ to represent the mean parameter of the Bernoulli. That's why we are switching all around.

Now we've chosen our $f$ (we only really had one choice, but that won't always be the case). We now need to choose the $g^{-1}$, which is called the "inverse link function." 

>#### An Aside on Inverses
>You might wonder why we work with an inverse function. There is a long answer to that, but the short answer is that people started off writing their models as $g(\theta_i) = X_i\beta$. We can easily go from $g$ to $g^{-1}$ and back. However, it is easist to work with $g^{-1}$ (i.e., the *inverse* link function), so that's what we'll use primarily. Authors sometimes get a little lax in their language and use "link function" to refer to an inverse link function. This is not technically correct, but it should be clear from the context what they mean.

So what should we choose for $g^{-1}$. Well, recall that $\pi_i = g^{-1}(X_i\beta)$. Also, recall that $\pi_i$ is the expected value of $y_i$ (i.e., the probability of an event) and therefore restricted to lie in the interval $\pi \in (0, 1)$. Thus, we need a function that takes values that can potentially lie along the entire real line (i.e., $X\beta$) and maps those values into the range $(0, 1)$. 

There are lots of possibilities here, but it turns out that cumulative distribution functions (cdfs) are really good at this particular thing. Take the normal distribution, for example. Its support lies along the entire real line (i.e., any real number is possible, if not probable) and, because it's a probability distribution, it's cdf ranges from zero to one. This is exactly what we need. Unfortately, there is no easy way to write the cdf for the normal distribution, so it's common to just use $\Phi$. So let's go with this choice as set $g^{-1} = \Phi$.

This gives us the model $Bernoulli(y_i | \pi_i) = \pi_i^{y_i}(1 - \pi_i)^{1 - y_i}$, where $\pi_i = \Phi(X_i\beta)$.

We're done. This is the probit model.

The only thing we need to do differently to obtain a logit rather than a probit is use the cdf of the logistic distribution instead of the normal cdf. We can write the cdf of the (standard) logistic distribution quite easily. It's just $\lambda(x) = \dfrac{1}{1 + e^{-x}}$. 

Thus, we can write the logit model as $y_i \sim Bernoulli(\pi_i)$, where $\pi_i = \lambda(X_i\beta) = \dfrac{1}{1 + e^{-X_i\beta}}$.

Note that we often write $\dfrac{1}{1 + e^{-X_i\beta}} = \text{logit}^{-1}(X_i\beta)$.

To get a feel for the model, let's simulate some data. In this case, we'll simulate `n = 100` observations and have one variable called `x1`.

Let's start with the probit.

```{r}
n <- 100
beta <- c(-1, 2)
x1 <- runif(n, -1, 1)
X <- cbind(1, x1)
pi <- pnorm(X%*%beta)  # the standard normal cdf
y <- rbinom(n, size = 1, pi)  # the Bernoulli is Binomial with size = 1
```

Now let's plot the data and the true probability of success.

```{r plot-simulated-data-probit}
plot(x1, y)
curve(pnorm(beta[1] + beta[2]*x), add = TRUE, lwd = 3, col = "red")
```

Now let's do the same thing, but with a logit.

```{r}
n <- 100
beta <- c(-1, 2)
x1 <- runif(n, -1, 1)
X <- cbind(1, x1)
pi <- plogis(X%*%beta)  # the standard logistic cdf
y <- rbinom(n, size = 1, pi)  # the Bernoulli is Binomial with size = 1
```

Now let's plot the data and the true probability of success.

```{r plot-simulated-data-logit}
plot(x1, y)
curve(plogis(beta[1] + beta[2]*x), add = TRUE, lwd = 3, col = "red")
```

Notice that the slope for the probit is a lot steeper than for the logit. This is because the cdfs are shaped differently. Let's see exactly what's going on by first plotting the distibutions. The logistic is in red and the normal is in black. 

```{r plot-logistic-normal}
par(mfrow = c(1, 2))

# pdfs
curve(dnorm(x), lwd = 3, col = "black", xlim = c(-3, 3))
curve(dlogis(x), add = TRUE, lwd = 3, col = "red")

# cdfs
curve(pnorm(x), lwd = 3, col = "black", xlim = c(-3, 3))
curve(plogis(x), add = TRUE, lwd = 3, col = "red")
```

Notice that the standard logistic distribution is much flatter (i.e., more diffuse) than the standard normal distribuion. There are paramaters that control the diffusion, so we can make them look more similar, but the standard versions are somewhat different. Notice the impact that this has on the cdfs--the cdf for the normal is much steeper. This simply means that the coefficients for the probit will be smaller than the coefficients for the logit model. In fact, the logit coefficients are about 1.81 times larger than the probit coefficients.

So how can we estimate a logit and or probit model? We will usually use the `glm()` function in R that works quite similarly to the `lm()` function that we used in PSC 531. However, `glm()` estimates a much broader class of models (i.e., a variety of $f$s and $g$s), so we'll need to be specific in telling `glm()` what to estimate.

### Example 1: Fake Data

```{r}
# simulate fake data
set.seed(3819704)
n <- 1000
beta <- c(0, 2)
x1 <- runif(n, -1, 1)
X <- cbind(1, x1)
pi <- plogis(X%*%beta)  # the standard logistic cdf
y <- rbinom(n, size = 1, pi)  # the Bernoulli is Binomial with size = 1

# estimate the model
m1 <- glm(y ~ x1, family = binomial(link = probit))  # probit
m2 <- glm(y ~ x1, family = binomial) # logit; note that (link = logit) is the default

# display estimates
library(arm)
display(m1)
display(m2)
```

Notice that, as expected, the logit coefficients are about 1.81 times larger than the probit coefficients. 

Interpreting Logit and Probit
-----------------------------

In the case of the linear model, the estimated coefficients (usually) have direct interpretations. This is not the case for logit and probit. You can infer sign and significance from the coefficients, but little in terms of magnitude. This will be the case for almost all of the models we'll examine in this class.

Rather than interpreting predicted probabilities, we'll compute substantively meaningful quantities of interest. There are many to choose from, but they are some function of the parameter estimates. In a later class, we'll discuss how to use simulation to get a confidence interval around these quantities of interest.

### Predicted Probabilites

In some cases, you just want to estimate the probability of an event. We can calculate this quite simply as $Pr(y_{new}) = f(X_{new}\hat{\beta})$, where the $new$ subscript simply indicates that we are predicting the probability of an event for an arbitrary case that we can set up however we like.

Let's continue our previous example and see how to calculate this. We'll do this in several steps.

```{r}
beta.hat <- coef(m1)  # pull out coefficients
X.new <- cbind(1, c(-1, .5, 0, .5, 1))  # set up prediction matrix
X.new  # see what X.new looks like
pr.prop <- pnorm(X.new%*%beta.hat)  # apply the inverse link function
pr.prop  # see what the preditions look like
```

You'll see that as our fake variable `x1` ranges from -1 to 1, the predicted probability of an event increases from 0.11 to 0.88.

### First-Derivatives

But often times, we are interested in how the probability of an event changes as our explanatory variables change.

## Example 1: O-rings and Temperature

In the `data` directory, you'll find the data set `challenger.csv`. Let's load the data and see what we have. (Make sure you've set the working directory to the course folder using something like `setwd("~/Dropbox/classes/adv-methods")`.)

```{r}
getwd()
d <- read.csv("data/challenger.csv")
head(d)
```
